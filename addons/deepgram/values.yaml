management:
  image: "yosefsporter/testdg:0.8.0"

api:
  image:
    repository: quay.io/deepgram/onprem-api
    pullPolicy: Always
    tag: release-240426
  resources:
    requests:
      cpu: 2
      memory: 12000Mi
    limits:
      cpu: 2
      memory: 16000Mi
  config: | 
    ### Keep in mind that all paths are in-container paths, and do not need to exist
    ### on the host machine.


    ### Configure license validation by passing in a DEEPGRAM_API_KEY environment variable
    ### See https://developers.deepgram.com/docs/deploy-deepgram-services#credentials
    [license]
      server_url = [ "https://license.deepgram.com" ]


    ### Configure how the API will listen for your requests
    [server]
      ### The base URL (prefix) for requests to the API.
      base_url = "/v1"
      ### The IP address to listen on. Since this is likely running in a Docker
      ### container, you will probably want to listen on all interfaces.
      host = "0.0.0.0"
      ### The port to listen on
      port = 8080

      ### How long to wait for a connection to a callback URL.
      callback_conn_timeout = "1s"
      ### How long to wait for a response to a callback URL.
      callback_timeout = "10s"

      ### How long to wait for a connection to a fetch URL.
      fetch_conn_timeout = "1s"
      ### How long to wait for a response to a fetch URL.
      fetch_timeout = "60s"


    ### By default, the API listens over HTTP. By passing both a certificate
    ### and a key file, the API will instead listen over HTTPS.
    ###
    ### This performs TLS termination only, and does not provide any
    ### additional authentication.
    [server.https]
      # cert_file = "/path/to/cert.pem"
      # key_file = "/path/to/key.pem"


    ### Specify custom DNS resolution options.
    # [resolver]
      ### Specify custom domain name server(s).
      ### Format is "{IP} {PORT} {PROTOCOL (tcp or udp)}"
      # nameservers = ["127.0.0.1 53 udp"]

      ### If specifying a custom DNS nameserver, set the DNS TTL value.
      # max_ttl = 10


    ### Limit the number of active requests handled by a single API container.
    ### If additional requests beyond the limit are sent, API will return
    ### a 429 HTTP status code. Default is no limit.
    [concurrency_limit]
      active_requests = 75


    ### Enable ancillary features
    [features]
      ### Enables topic detection *if* a valid topic detection model is available
      topic_detection = false # or true

      ### Enables summarization *if* a valid summarization model is available
      summarization = false # or true

      ### If API is receiving requests faster than Engine can process them, a request
      ### queue will form. By default, this queue is stored in memory. Under high load,
      ### the queue may grow too large and cause Out-Of-Memory errors. To avoid this,
      ### set a disk_buffer_path to buffer the overflow on the request queue to disk.
      ###
      ### WARN: This is only to temporarily buffer requests during high load.
      ### If there is not enough Engine capacity to process the queued requests over time,
      ### the queue (and response time) will grow indefniitely.
      # disk_buffer_path: Option<PathBuf>,


    ### Configure the backend pool of speech engines (generically referred to as
    ### "drivers" here). The API will load-balance among drivers in the standard
    ### pool; if one standard driver fails, the next one will be tried.
    ###
    ### Each driver URL will have its hostname resolved to an IP address. If a domain
    ### name resolves to multiple IP addresses, the API will load-balance across each
    ### IP address.
    ###
    ### This behavior is provided for convenience, and in a production environment
    ### other tools can be used, such as HAProxy.
    ###
    ### Below is a new Speech Engine ("driver") in the "standard" pool.
    [[driver_pool.standard]]
      ### Host to connect to. If you are using a different method of orchestrating,
      ### then adjust the IP address accordingly.
      ###
      ### WARN: This must be HTTPS.
      url = "https://deepgram-engine.default.svc.cluster.local:8080/v2" # Docker Compose. This is going to change to engine from Porter

      ### Factor to increase the timeout by for each additional retry (for
      ### exponential backoff).
      timeout_backoff = 1.2

      ### Before attempting a retry, sleep for this long (in seconds)
      retry_sleep = "2s"
      ### Factor to increase the retry sleep by for each additional retry (for
      ### exponential backoff).
      retry_backoff = 1.6

      ### Maximum response to deserialize from Driver (in bytes)
      max_response_size = 1073741824 # 1GB
  

engine:
  image:
    repository: quay.io/deepgram/onprem-engine
    pullPolicy: Always
    tag: release-240426
  resources:
    requests:
      cpu: 2
      memory: 12000Mi
      nvidiaGpu: "1"
    limits:
      cpu: 4
      memory: 16000Mi
      nvidiaGpu: "1"
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"
  config: |
    ### Keep in mind that all paths are in-container paths and do not need to exist
    ### on the host machine.

    ### Limit the number of active requests handled by a single Engine container.
    ### Engine will reject additional requests from API beyond this limit, and the
    ### API container will continue with the retry logic configured in `api.toml`.
    ###
    ### The default is no limit.
    # max_active_requests = 30


    ### Configure license validation by passing in a DEEPGRAM_API_KEY environment variable
    ### See https://developers.deepgram.com/docs/deploy-deepgram-services#credentials
    [license]
      server_url = [ "https://license.deepgram.com" ]


    ### Configure the server to listen for requests from the API.
    [server]
      ### The IP address to listen on. Since this is likely running in a Docker
      ### container, you will probably want to listen on all interfaces.
      host = "0.0.0.0"
      ### The port to listen on
      port = 8080


    ### To support metrics we need to expose an Engine endpoint.
    ### See https://developers.deepgram.com/docs/metrics-guide#deepgram-engine
    [metrics_server]
      host = "0.0.0.0"
      port = 9991


    ### Inference models. You can place these in one or multiple directories.
    [model_manager]
      search_paths = ["/data/models"]


    ### Enable ancillary features
    [features]
      ### Allow multichannel requests by setting this to true, set to false to disable
      multichannel = true # or false
      ### Enables language detection *if* a valid language detection model is available
      language_detection = true # or false


    ### Size of audio chunks to process in seconds.
    [chunking.batch]
      ### Batch min/max default is 7/10s
      # min_duration = 7.0
      # max_duration = 10.0
    [chunking.streaming]
      ### Streaming min/max default is 3/5s
      # min_duration = 3.0
      # max_duration = 5.0

      ### How often to return interim results, in seconds. Default is 1.0s.
      ###
      ### This value may be lowered to increase the frequency of interim results.
      ### However, this also causes a signficant decrease in number of concurrent
      ### streams supported by a single GPU. Please contact your Deepgram Account 
      ### representative for more details.
      # step = 1.0


    ### Engine will automatically enable half precision operations if your GPU supports
    ### them. You can explicitly enable or disable this behavior with the state parameter
    ### which supports enabled, disabled, and auto (the default).
    [half_precision]
      # state = "disabled" # or "enabled" or "auto"


modelURLs: # this should be a list of links to download models from as new line separated strings

deepgramAPIKey: "your api key"

diskBufferPath: "/data/disk-buffer"

modelPath: "/data/models"

efsFileSystemId: "this should be the efs file system id created in the clients cluster"

apiNodeGroupID: # the ID of the api node group where the api should run

engineNodeGroupID: # the ID of the node group dedicated to the engine if any

imageCredentials:
  username: deepgram+hash
  secret: secret
  registry: quay.io