vllmImage: "docker.io/vllm/vllm-openai:latest"

huggingFaceToken: # a hugging face token if it is required by the model 

modelDir: "/model"

modelID: "mistralai/Mistral-7B-Instruct-v0.2"

gpuMemoryUtilization: 0.98

maxModelLen: 26016

dType: half

efsFileSystemId: # efs file system id created in the clients cluster, required

tensorParallelSize: 1

additionalAnnotations: {}

additionalLabels:
  porter.run/kube-image-keeper-enabled: "true"

nodeGroupId: # the node group id where the model will be deployed

readinessProbe:
  failureThreshold: 60
  httpGet:
    path: /health
    port: 8000
    scheme: HTTP
  initialDelaySeconds: 10
  periodSeconds: 2
  successThreshold: 1
  timeoutSeconds: 2

resources:
  requests:
    cpu: 2
    memory: 12000Mi
    nvidiaGpu: "1"
  limits:
    cpu: 4
    memory: 40000Mi
    nvidiaGpu: "1"

tolerations:
  - key: "nvidia.com/gpu"
    operator: "Exists"
    effect: "NoSchedule"

autoscaling:
  enabled: false 
  minReplicas: 0 
  maxReplicas: 10
  scaledownPeriod: 300 # the time in seconds to wait before scaling down the deployment after the last request
  targetConcurrency: 100 # the target concurrent connections per replica

# Metrics configuration for Prometheus scraping
metrics:
  enabled: false # Whether to enable metrics scraping