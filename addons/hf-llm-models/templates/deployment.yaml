apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    llm-model: {{ .Release.Name }}-hf-llm
  annotations:
    porter.run/hf-llm-model-version: "{{ .Chart.Version }}"
  name: {{ .Release.Name }}-hf-llm
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      llm-model: {{ .Release.Name }}-hf-llm
  template:
    metadata:
      {{- with .Values.additionalAnnotations }}
      annotations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      labels:
        llm-model: {{ .Release.Name }}-hf-llm
        {{- with .Values.additionalLabels }}
          {{- toYaml . | nindent 8 }}
        {{- end }}
    spec:
      tolerations:
        - key: "removable"
          operator: "Equal"
          value: "true"
          effect: "NoSchedule"
      {{- if .Values.tolerations }}
        {{- toYaml .Values.tolerations | nindent 8 }}
      {{- end }}
      {{- if .Values.nodeGroupId }}
        - effect: NoSchedule
          key: "porter.run/node-group-id"
          operator: "Equal"
          value: "{{ .Values.nodeGroupId }}"
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: porter.run/node-group-id
                    operator: In
                    values:
                      - {{ .Values.nodeGroupId | quote }}
      {{- end }}
      containers:
      - command:
        - python3
        - -m
        - vllm.entrypoints.openai.api_server
        - --model
        - {{ .Values.modelID }}
      {{- if .Values.quantization }}
        - --quantization
        - {{ .Values.quantization }}
      {{- end }}
        - --download-dir
        - {{ .Values.modelDir }}
        - --gpu-memory-utilization={{ printf "%.2f" .Values.gpuMemoryUtilization }}
      {{- if .Values.dType}}
        - --dtype
        - {{ .Values.dType }}
      {{- end }}
        - --tensor-parallel-size={{ .Values.tensorParallelSize }}
      {{- if .Values.maxModelLen }}
        - --max-model-len={{ .Values.maxModelLen }}
      {{- end }}
      {{- if .Values.extraArgs }}
        {{- range .Values.extraArgs }}
        - {{ . }}
        {{- end }}
      {{- end }}
        image: {{ .Values.vllmImage }}
        imagePullPolicy: IfNotPresent
        env:
        - name: HF_TOKEN
          value: {{ .Values.huggingFaceToken }}
        - name: NCCL_DEBUG
          value: INFO
        ports:
        - containerPort: 8000
          protocol: TCP
          name: https
        {{- with .Values.readinessProbe }}
        readinessProbe:
          {{- toYaml . | nindent 12 }}
        {{- end }}
        resources:
          requests:
              {{- if .Values.resources.requests.cpu }}
            cpu: {{ .Values.resources.requests.cpu }}
              {{- end }}
              {{- if .Values.resources.requests.memory }}
            memory: {{ .Values.resources.requests.memory }}
              {{- end }}
              {{- if .Values.resources.requests.nvidiaGpu }}
            nvidia.com/gpu: {{ .Values.resources.requests.nvidiaGpu }}
              {{- end }}
          limits:
              {{- if .Values.resources.limits.cpu }}
            cpu: {{ .Values.resources.limits.cpu }}
              {{- end }}
              {{- if .Values.resources.limits.memory }}
            memory: {{ .Values.resources.limits.memory }}
              {{- end }}
              {{- if .Values.resources.limits.nvidiaGpu }}
            nvidia.com/gpu: {{ .Values.resources.limits.nvidiaGpu }}
              {{- end }}
        volumeMounts:
        - name: dshm
          mountPath: /dev/shm
        - name: model-volume
          mountPath: {{ .Values.modelDir }}
        name: vllm
        securityContext:
          allowPrivilegeEscalation: false
      terminationGracePeriodSeconds: 10
      volumes:
      - name: dshm
        emptyDir:
          medium: Memory
          sizeLimit: 4Gi
      - name: model-volume
        persistentVolumeClaim:
          claimName: "{{ .Release.Name }}-hf-llm"
